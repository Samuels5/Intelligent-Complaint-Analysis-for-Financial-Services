{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22a54cb0",
   "metadata": {},
   "source": [
    "# Task 2: Text Chunking, Embedding, and Vector Store Indexing\n",
    "\n",
    "## Text Chunking, Embedding, and Vector Store for RAG System\n",
    "\n",
    "This notebook implements the text chunking strategy, generates embeddings, and creates a vector store for efficient semantic search.\n",
    "\n",
    "**Objectives:**\n",
    "- Implement text chunking strategy for long complaint narratives\n",
    "- Generate embeddings using sentence transformers\n",
    "- Create and populate a vector store (FAISS)\n",
    "- Store metadata for traceability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42472a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import pickle\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# Check if GPU is available\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1246a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Text Chunking, Embedding, and Vector Store Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ada874",
   "metadata": {},
   "source": [
    "## 1. Load Cleaned Data\n",
    "\n",
    "First, let's load the cleaned complaint data from Task 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502decaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned data from Task 1\n",
    "data_path = \"../data/filtered_complaints.csv\"\n",
    "\n",
    "if os.path.exists(data_path):\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"✅ Loaded cleaned data: {df.shape}\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # Check text lengths\n",
    "    print(f\"\\nText length statistics:\")\n",
    "    print(df['cleaned_length'].describe())\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Cleaned data not found. Please run Task 1 first.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde985ea",
   "metadata": {},
   "source": [
    "## 2. Text Chunking Strategy\n",
    "\n",
    "Long narratives are often ineffective when embedded as a single vector. We'll implement a recursive character text splitter to break narratives into smaller, semantically meaningful chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77252e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecursiveCharacterTextSplitter:\n",
    "    \"\"\"\n",
    "    A simple recursive character text splitter that breaks text into chunks\n",
    "    while trying to preserve semantic boundaries.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 500, chunk_overlap: int = 100, separators: List[str] = None):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.separators = separators or [\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \";\", \" \", \"\"]\n",
    "    \n",
    "    def split_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Split text into chunks using recursive character splitting.\"\"\"\n",
    "        if len(text) <= self.chunk_size:\n",
    "            return [text]\n",
    "        \n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        \n",
    "        # Split by separators in order of preference\n",
    "        for separator in self.separators:\n",
    "            if separator in text:\n",
    "                parts = text.split(separator)\n",
    "                for i, part in enumerate(parts):\n",
    "                    if len(current_chunk) + len(part) + len(separator) <= self.chunk_size:\n",
    "                        current_chunk += part\n",
    "                        if i < len(parts) - 1:  # Don't add separator after last part\n",
    "                            current_chunk += separator\n",
    "                    else:\n",
    "                        if current_chunk:\n",
    "                            chunks.append(current_chunk.strip())\n",
    "                        current_chunk = part\n",
    "                        if i < len(parts) - 1:\n",
    "                            current_chunk += separator\n",
    "                \n",
    "                if current_chunk:\n",
    "                    chunks.append(current_chunk.strip())\n",
    "                break\n",
    "        \n",
    "        # Handle overlap\n",
    "        final_chunks = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            if i == 0:\n",
    "                final_chunks.append(chunk)\n",
    "            else:\n",
    "                # Add overlap from previous chunk\n",
    "                overlap_text = chunks[i-1][-self.chunk_overlap:] if len(chunks[i-1]) > self.chunk_overlap else chunks[i-1]\n",
    "                final_chunks.append(overlap_text + \" \" + chunk)\n",
    "        \n",
    "        return [chunk for chunk in final_chunks if chunk.strip()]\n",
    "\n",
    "# Test the chunking strategy\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "\n",
    "# Test with a sample narrative\n",
    "if not df.empty:\n",
    "    sample_text = df['Consumer complaint narrative'].iloc[0]\n",
    "    print(f\"Original text length: {len(sample_text)}\")\n",
    "    print(f\"Original text: {sample_text[:200]}...\")\n",
    "    \n",
    "    chunks = splitter.split_text(sample_text)\n",
    "    print(f\"\\nNumber of chunks: {len(chunks)}\")\n",
    "    for i, chunk in enumerate(chunks[:3]):  # Show first 3 chunks\n",
    "        print(f\"Chunk {i+1} (length: {len(chunk)}): {chunk[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e371d5bf",
   "metadata": {},
   "source": [
    "## 3. Embedding Model Selection\n",
    "\n",
    "We'll use the `sentence-transformers/all-MiniLM-L6-v2` model for generating embeddings. This model provides a good balance between performance and computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a1626b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the embedding model\n",
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "print(f\"Loading embedding model: {model_name}\")\n",
    "\n",
    "# Load the model\n",
    "embedding_model = SentenceTransformer(model_name)\n",
    "embedding_model.to(device)\n",
    "\n",
    "print(f\"✅ Model loaded successfully!\")\n",
    "print(f\"Model max sequence length: {embedding_model.max_seq_length}\")\n",
    "print(f\"Model embedding dimension: {embedding_model.get_sentence_embedding_dimension()}\")\n",
    "\n",
    "# Test embedding generation\n",
    "test_text = \"I am having issues with my credit card billing\"\n",
    "test_embedding = embedding_model.encode([test_text])\n",
    "print(f\"\\nTest embedding shape: {test_embedding.shape}\")\n",
    "print(f\"Sample embedding values: {test_embedding[0][:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2b7f8e",
   "metadata": {},
   "source": [
    "## 4. Process All Complaint Narratives\n",
    "\n",
    "Now we'll process all complaint narratives through our chunking strategy and prepare them for embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3249db6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all complaint narratives into chunks\n",
    "def process_complaints_to_chunks(df, splitter):\n",
    "    \"\"\"\n",
    "    Process all complaint narratives into chunks with metadata.\n",
    "    \"\"\"\n",
    "    all_chunks = []\n",
    "    chunk_metadata = []\n",
    "    \n",
    "    print(f\"Processing {len(df)} complaints into chunks...\")\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        narrative = row['Consumer complaint narrative']\n",
    "        chunks = splitter.split_text(narrative)\n",
    "        \n",
    "        for chunk_idx, chunk in enumerate(chunks):\n",
    "            all_chunks.append(chunk)\n",
    "            chunk_metadata.append({\n",
    "                'complaint_id': row.get('Complaint ID', idx),\n",
    "                'chunk_id': f\"{idx}_{chunk_idx}\",\n",
    "                'chunk_index': chunk_idx,\n",
    "                'total_chunks': len(chunks),\n",
    "                'product': row['Product'],\n",
    "                'issue': row['Issue'],\n",
    "                'company': row.get('Company', 'Unknown'),\n",
    "                'date_received': row.get('Date received', ''),\n",
    "                'original_text_length': len(narrative),\n",
    "                'chunk_length': len(chunk)\n",
    "            })\n",
    "    \n",
    "    return all_chunks, chunk_metadata\n",
    "\n",
    "# Process the complaints\n",
    "all_chunks, chunk_metadata = process_complaints_to_chunks(df, splitter)\n",
    "\n",
    "print(f\"\\n✅ Processing complete!\")\n",
    "print(f\"Total chunks created: {len(all_chunks)}\")\n",
    "print(f\"Average chunks per complaint: {len(all_chunks) / len(df):.2f}\")\n",
    "\n",
    "# Analyze chunk statistics\n",
    "chunk_lengths = [len(chunk) for chunk in all_chunks]\n",
    "print(f\"\\nChunk length statistics:\")\n",
    "print(f\"Min: {min(chunk_lengths)}\")\n",
    "print(f\"Max: {max(chunk_lengths)}\")\n",
    "print(f\"Mean: {np.mean(chunk_lengths):.2f}\")\n",
    "print(f\"Median: {np.median(chunk_lengths):.2f}\")\n",
    "\n",
    "# Visualize chunk lengths\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(chunk_lengths, bins=50, alpha=0.7, edgecolor='black')\n",
    "plt.title('Distribution of Chunk Lengths')\n",
    "plt.xlabel('Chunk Length (characters)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(np.mean(chunk_lengths), color='red', linestyle='--', label=f'Mean: {np.mean(chunk_lengths):.0f}')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b220daf0",
   "metadata": {},
   "source": [
    "## 5. Generate Embeddings\n",
    "\n",
    "Now we'll generate embeddings for all chunks using our selected model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967ba6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for all chunks\n",
    "def generate_embeddings_batch(chunks, model, batch_size=32):\n",
    "    \"\"\"\n",
    "    Generate embeddings for chunks in batches to manage memory.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    print(f\"Generating embeddings for {len(chunks)} chunks...\")\n",
    "    \n",
    "    for i in tqdm(range(0, len(chunks), batch_size)):\n",
    "        batch = chunks[i:i + batch_size]\n",
    "        batch_embeddings = model.encode(batch, convert_to_numpy=True)\n",
    "        embeddings.extend(batch_embeddings)\n",
    "    \n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Generate embeddings\n",
    "print(\"Starting embedding generation...\")\n",
    "embeddings = generate_embeddings_batch(all_chunks, embedding_model, batch_size=32)\n",
    "\n",
    "print(f\"\\n✅ Embedding generation complete!\")\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"Embedding dimension: {embeddings.shape[1]}\")\n",
    "print(f\"Total embeddings: {embeddings.shape[0]}\")\n",
    "\n",
    "# Save embeddings and metadata for later use\n",
    "embeddings_path = \"../vector_store/embeddings.npy\"\n",
    "metadata_path = \"../vector_store/metadata.pkl\"\n",
    "\n",
    "os.makedirs(\"../vector_store\", exist_ok=True)\n",
    "\n",
    "np.save(embeddings_path, embeddings)\n",
    "with open(metadata_path, 'wb') as f:\n",
    "    pickle.dump(chunk_metadata, f)\n",
    "\n",
    "print(f\"✅ Embeddings saved to: {embeddings_path}\")\n",
    "print(f\"✅ Metadata saved to: {metadata_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13707fa3",
   "metadata": {},
   "source": [
    "## 6. Create FAISS Vector Store\n",
    "\n",
    "We'll create a FAISS index for efficient similarity search and store it for use in the RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9da632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FAISS index\n",
    "def create_faiss_index(embeddings):\n",
    "    \"\"\"\n",
    "    Create a FAISS index for similarity search.\n",
    "    \"\"\"\n",
    "    dimension = embeddings.shape[1]\n",
    "    \n",
    "    # Use IndexFlatIP for cosine similarity (inner product after normalization)\n",
    "    # Normalize embeddings for cosine similarity\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    \n",
    "    # Create index\n",
    "    index = faiss.IndexFlatIP(dimension)\n",
    "    \n",
    "    # Add embeddings to index\n",
    "    index.add(embeddings.astype('float32'))\n",
    "    \n",
    "    return index\n",
    "\n",
    "# Create the FAISS index\n",
    "print(\"Creating FAISS index...\")\n",
    "faiss_index = create_faiss_index(embeddings.copy())\n",
    "\n",
    "print(f\"✅ FAISS index created!\")\n",
    "print(f\"Index dimension: {faiss_index.d}\")\n",
    "print(f\"Total vectors in index: {faiss_index.ntotal}\")\n",
    "\n",
    "# Save the FAISS index\n",
    "index_path = \"../vector_store/faiss_index.bin\"\n",
    "faiss.write_index(faiss_index, index_path)\n",
    "print(f\"✅ FAISS index saved to: {index_path}\")\n",
    "\n",
    "# Save chunks for retrieval\n",
    "chunks_path = \"../vector_store/chunks.pkl\"\n",
    "with open(chunks_path, 'wb') as f:\n",
    "    pickle.dump(all_chunks, f)\n",
    "print(f\"✅ Chunks saved to: {chunks_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f5faf7",
   "metadata": {},
   "source": [
    "## 7. Test Vector Store Retrieval\n",
    "\n",
    "Let's test our vector store by performing some sample queries to ensure everything works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41913c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test retrieval functionality\n",
    "def test_retrieval(query, index, chunks, metadata, model, k=5):\n",
    "    \"\"\"\n",
    "    Test retrieval functionality with a sample query.\n",
    "    \"\"\"\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Encode query\n",
    "    query_embedding = model.encode([query])\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "    \n",
    "    # Search\n",
    "    scores, indices = index.search(query_embedding.astype('float32'), k)\n",
    "    \n",
    "    print(f\"Top {k} results:\")\n",
    "    for i, (score, idx) in enumerate(zip(scores[0], indices[0])):\n",
    "        chunk = chunks[idx]\n",
    "        meta = metadata[idx]\n",
    "        \n",
    "        print(f\"\\n{i+1}. Score: {score:.4f}\")\n",
    "        print(f\"   Product: {meta['product']}\")\n",
    "        print(f\"   Issue: {meta['issue']}\")\n",
    "        print(f\"   Chunk: {chunk[:200]}...\")\n",
    "        print(f\"   Metadata: Complaint ID: {meta['complaint_id']}, Chunk {meta['chunk_index']+1}/{meta['total_chunks']}\")\n",
    "\n",
    "# Test with sample queries\n",
    "test_queries = [\n",
    "    \"Problems with credit card billing\",\n",
    "    \"Issues with personal loan payments\",\n",
    "    \"BNPL payment problems\",\n",
    "    \"Savings account access issues\",\n",
    "    \"Money transfer delays\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    test_retrieval(query, faiss_index, all_chunks, chunk_metadata, embedding_model, k=3)\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c3d4d0",
   "metadata": {},
   "source": [
    "## 8. Summary and Next Steps\n",
    "\n",
    "### Chunking Strategy Summary\n",
    "- **Chunk Size**: 500 characters with 100 character overlap\n",
    "- **Separators**: Prioritized by semantic boundaries (paragraphs, sentences, words)\n",
    "- **Total Chunks**: Generated from complaint narratives\n",
    "- **Average Chunks per Complaint**: Calculated based on text length\n",
    "\n",
    "### Embedding Model Choice\n",
    "- **Model**: `sentence-transformers/all-MiniLM-L6-v2`\n",
    "- **Rationale**: \n",
    "  - Good balance between performance and computational efficiency\n",
    "  - 384-dimensional embeddings\n",
    "  - Suitable for semantic similarity tasks\n",
    "  - Fast inference time\n",
    "\n",
    "### Vector Store Configuration\n",
    "- **Index Type**: FAISS IndexFlatIP (Inner Product for cosine similarity)\n",
    "- **Normalization**: L2 normalization for cosine similarity\n",
    "- **Metadata Storage**: Complete traceability to original complaints\n",
    "\n",
    "### Files Created\n",
    "1. `../vector_store/embeddings.npy` - Raw embeddings array\n",
    "2. `../vector_store/metadata.pkl` - Chunk metadata with traceability\n",
    "3. `../vector_store/faiss_index.bin` - FAISS index for similarity search\n",
    "4. `../vector_store/chunks.pkl` - Text chunks for retrieval\n",
    "\n",
    "### Next Steps\n",
    "The vector store is now ready for Task 3: Building the RAG Core Logic and Evaluation. The retrieval system can efficiently find relevant complaint chunks based on semantic similarity to user queries."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
