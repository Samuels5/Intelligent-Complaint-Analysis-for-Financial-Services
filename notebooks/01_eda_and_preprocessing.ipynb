{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a4c2bef",
   "metadata": {},
   "source": [
    "# Task 1: Exploratory Data Analysis and Data Preprocessing\n",
    "\n",
    "## Intelligent Complaint Analysis for Financial Services\n",
    "\n",
    "This notebook performs comprehensive exploratory data analysis and preprocessing of the CFPB complaint dataset for the RAG-powered chatbot project.\n",
    "\n",
    "**Objectives:**\n",
    "- Load and explore the CFPB complaint dataset\n",
    "- Analyze complaint distributions across financial products\n",
    "- Perform text analysis on complaint narratives\n",
    "- Clean and filter data for the specified products\n",
    "- Prepare data for embedding and vector store creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "344dc722",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5968/3940238351.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpress\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph_objects\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mgo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mplotly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmake_subplots\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "import os\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import textstat\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "from datetime import datetime\n",
    "import zipfile\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# Configure plotting\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set up NLTK\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    \n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c01cf8",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Exploration\n",
    "\n",
    "We'll load the CFPB complaint dataset and perform initial exploration to understand the structure and content of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc9e7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CFPB complaint dataset\n",
    "# URL for the Consumer Complaint Database\n",
    "data_url = \"https://files.consumerfinance.gov/ccdb/complaints.csv.zip\"\n",
    "\n",
    "# Check if data already exists\n",
    "data_path = \"../data/complaints.csv\"\n",
    "if not os.path.exists(data_path):\n",
    "    print(\"Downloading CFPB complaint dataset...\")\n",
    "    \n",
    "    # Download the dataset\n",
    "    response = requests.get(data_url)\n",
    "    if response.status_code == 200:\n",
    "        # Extract the CSV from the zip file\n",
    "        with zipfile.ZipFile(BytesIO(response.content)) as zip_file:\n",
    "            zip_file.extractall(\"../data/\")\n",
    "        print(\"Dataset downloaded and extracted successfully!\")\n",
    "    else:\n",
    "        print(f\"Failed to download dataset. Status code: {response.status_code}\")\n",
    "else:\n",
    "    print(\"Dataset already exists locally.\")\n",
    "\n",
    "# Load the dataset\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv(data_path, low_memory=False)\n",
    "\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0fb88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial data exploration\n",
    "print(\"=== DATASET OVERVIEW ===\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(\"\\n=== FIRST FEW ROWS ===\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\n=== DATA TYPES ===\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\n=== MISSING VALUES ===\")\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_values,\n",
    "    'Missing Percentage': missing_percentage\n",
    "}).sort_values('Missing Count', ascending=False)\n",
    "print(missing_df[missing_df['Missing Count'] > 0])\n",
    "\n",
    "print(\"\\n=== BASIC STATISTICS ===\")\n",
    "print(df.describe(include='all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750fa7cf",
   "metadata": {},
   "source": [
    "## 2. Product Distribution Analysis\n",
    "\n",
    "Let's analyze the distribution of complaints across different financial products, with focus on our target products: Credit card, Personal loan, Buy Now Pay Later (BNPL), Savings account, and Money transfers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752fb1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our target products\n",
    "target_products = [\n",
    "    'Credit card',\n",
    "    'Personal loan', \n",
    "    'Buy Now, Pay Later (BNPL)',\n",
    "    'Savings account',\n",
    "    'Money transfers'\n",
    "]\n",
    "\n",
    "# Check unique products in the dataset\n",
    "print(\"=== ALL PRODUCTS IN DATASET ===\")\n",
    "product_counts = df['Product'].value_counts()\n",
    "print(product_counts)\n",
    "\n",
    "print(f\"\\nTotal unique products: {len(product_counts)}\")\n",
    "print(f\"Total complaints: {len(df)}\")\n",
    "\n",
    "# Map similar product names to our target products\n",
    "product_mapping = {\n",
    "    'Credit card or prepaid card': 'Credit card',\n",
    "    'Payday loan, title loan, or personal loan': 'Personal loan',\n",
    "    'Payday loan, title loan, personal loan, or advance loan': 'Personal loan',\n",
    "    'Money transfer, virtual currency, or money service': 'Money transfers',\n",
    "    'Money transfers': 'Money transfers',\n",
    "    'Checking or savings account': 'Savings account',\n",
    "    'Bank account or service': 'Savings account',\n",
    "    'Credit card': 'Credit card',\n",
    "    'Personal loan': 'Personal loan',\n",
    "    'Buy Now, Pay Later (BNPL)': 'Buy Now, Pay Later (BNPL)',\n",
    "    'Savings account': 'Savings account'\n",
    "}\n",
    "\n",
    "# Apply mapping and filter\n",
    "df['Product_mapped'] = df['Product'].map(product_mapping)\n",
    "df['Product_mapped'] = df['Product_mapped'].fillna('Other')\n",
    "\n",
    "print(\"\\n=== MAPPED PRODUCT DISTRIBUTION ===\")\n",
    "mapped_counts = df['Product_mapped'].value_counts()\n",
    "print(mapped_counts)\n",
    "\n",
    "# Filter for target products\n",
    "df_filtered = df[df['Product_mapped'].isin(target_products)].copy()\n",
    "print(f\"\\nFiltered dataset shape: {df_filtered.shape}\")\n",
    "print(f\"Percentage of original data retained: {len(df_filtered)/len(df)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751d80a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize product distribution\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('All Products (Top 10)', 'Target Products Distribution', \n",
    "                   'Target Products Over Time', 'Complaints by Company Type'),\n",
    "    specs=[[{\"type\": \"bar\"}, {\"type\": \"pie\"}],\n",
    "           [{\"type\": \"scatter\"}, {\"type\": \"bar\"}]]\n",
    ")\n",
    "\n",
    "# Top 10 products\n",
    "top_products = product_counts.head(10)\n",
    "fig.add_trace(\n",
    "    go.Bar(x=top_products.index, y=top_products.values, name=\"All Products\"),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Target products pie chart\n",
    "target_counts = df_filtered['Product_mapped'].value_counts()\n",
    "fig.add_trace(\n",
    "    go.Pie(labels=target_counts.index, values=target_counts.values, name=\"Target Products\"),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Time series of complaints for target products\n",
    "df_filtered['Date received'] = pd.to_datetime(df_filtered['Date received'])\n",
    "df_filtered['Year'] = df_filtered['Date received'].dt.year\n",
    "yearly_complaints = df_filtered.groupby(['Year', 'Product_mapped']).size().reset_index(name='Count')\n",
    "\n",
    "for product in target_products:\n",
    "    product_data = yearly_complaints[yearly_complaints['Product_mapped'] == product]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=product_data['Year'], y=product_data['Count'], \n",
    "                  name=product, mode='lines+markers'),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "# Company distribution (top 10)\n",
    "company_counts = df_filtered['Company'].value_counts().head(10)\n",
    "fig.add_trace(\n",
    "    go.Bar(x=company_counts.values, y=company_counts.index, \n",
    "           orientation='h', name=\"Top Companies\"),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=800, title_text=\"Complaint Distribution Analysis\")\n",
    "fig.update_xaxes(tickangle=45, row=1, col=1)\n",
    "fig.show()\n",
    "\n",
    "print(\"=== TARGET PRODUCTS SUMMARY ===\")\n",
    "for product in target_products:\n",
    "    count = target_counts.get(product, 0)\n",
    "    percentage = (count / len(df_filtered)) * 100\n",
    "    print(f\"{product}: {count:,} complaints ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4ffe3e",
   "metadata": {},
   "source": [
    "## 3. Text Analysis of Complaint Narratives\n",
    "\n",
    "Now let's analyze the consumer complaint narratives to understand text characteristics and identify patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4d4e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze complaint narratives\n",
    "print(\"=== NARRATIVE ANALYSIS ===\")\n",
    "\n",
    "# Check for missing narratives\n",
    "narrative_col = 'Consumer complaint narrative'\n",
    "total_complaints = len(df_filtered)\n",
    "complaints_with_narratives = df_filtered[narrative_col].notna().sum()\n",
    "complaints_without_narratives = total_complaints - complaints_with_narratives\n",
    "\n",
    "print(f\"Total complaints: {total_complaints:,}\")\n",
    "print(f\"Complaints with narratives: {complaints_with_narratives:,} ({complaints_with_narratives/total_complaints*100:.1f}%)\")\n",
    "print(f\"Complaints without narratives: {complaints_without_narratives:,} ({complaints_without_narratives/total_complaints*100:.1f}%)\")\n",
    "\n",
    "# Filter out complaints without narratives\n",
    "df_with_narratives = df_filtered[df_filtered[narrative_col].notna()].copy()\n",
    "print(f\"\\nFiltered dataset with narratives: {len(df_with_narratives):,}\")\n",
    "\n",
    "# Analyze text length\n",
    "df_with_narratives['narrative_length'] = df_with_narratives[narrative_col].str.len()\n",
    "df_with_narratives['word_count'] = df_with_narratives[narrative_col].str.split().str.len()\n",
    "df_with_narratives['sentence_count'] = df_with_narratives[narrative_col].str.split('.').str.len()\n",
    "\n",
    "print(\"\\n=== TEXT LENGTH STATISTICS ===\")\n",
    "print(\"Character count:\")\n",
    "print(df_with_narratives['narrative_length'].describe())\n",
    "print(\"\\nWord count:\")\n",
    "print(df_with_narratives['word_count'].describe())\n",
    "print(\"\\nSentence count:\")\n",
    "print(df_with_narratives['sentence_count'].describe())\n",
    "\n",
    "# Identify very short and very long narratives\n",
    "print(\"\\n=== EXTREME CASES ===\")\n",
    "short_threshold = 50  # characters\n",
    "long_threshold = 5000  # characters\n",
    "\n",
    "very_short = df_with_narratives[df_with_narratives['narrative_length'] <= short_threshold]\n",
    "very_long = df_with_narratives[df_with_narratives['narrative_length'] >= long_threshold]\n",
    "\n",
    "print(f\"Very short narratives (â‰¤{short_threshold} chars): {len(very_short):,} ({len(very_short)/len(df_with_narratives)*100:.1f}%)\")\n",
    "print(f\"Very long narratives (â‰¥{long_threshold} chars): {len(very_long):,} ({len(very_long)/len(df_with_narratives)*100:.1f}%)\")\n",
    "\n",
    "# Show examples\n",
    "print(\"\\n=== EXAMPLE SHORT NARRATIVE ===\")\n",
    "if len(very_short) > 0:\n",
    "    print(very_short[narrative_col].iloc[0])\n",
    "    \n",
    "print(\"\\n=== EXAMPLE LONG NARRATIVE (truncated) ===\")\n",
    "if len(very_long) > 0:\n",
    "    print(very_long[narrative_col].iloc[0][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536f36ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize text length distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Character count distribution\n",
    "axes[0, 0].hist(df_with_narratives['narrative_length'], bins=50, alpha=0.7, color='skyblue')\n",
    "axes[0, 0].set_title('Distribution of Character Count')\n",
    "axes[0, 0].set_xlabel('Character Count')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Word count distribution\n",
    "axes[0, 1].hist(df_with_narratives['word_count'], bins=50, alpha=0.7, color='lightgreen')\n",
    "axes[0, 1].set_title('Distribution of Word Count')\n",
    "axes[0, 1].set_xlabel('Word Count')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Box plot of word count by product\n",
    "df_with_narratives.boxplot(column='word_count', by='Product_mapped', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Word Count by Product')\n",
    "axes[1, 0].set_xlabel('Product')\n",
    "axes[1, 0].set_ylabel('Word Count')\n",
    "\n",
    "# Scatter plot: word count vs character count\n",
    "axes[1, 1].scatter(df_with_narratives['word_count'], df_with_narratives['narrative_length'], \n",
    "                  alpha=0.1, s=1)\n",
    "axes[1, 1].set_title('Word Count vs Character Count')\n",
    "axes[1, 1].set_xlabel('Word Count')\n",
    "axes[1, 1].set_ylabel('Character Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Word frequency analysis\n",
    "print(\"=== WORD FREQUENCY ANALYSIS ===\")\n",
    "\n",
    "# Function to clean and tokenize text\n",
    "def clean_and_tokenize(text):\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    # Convert to lowercase and remove special characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words and len(token) > 2]\n",
    "    return tokens\n",
    "\n",
    "# Sample a subset for word frequency analysis (for performance)\n",
    "sample_size = min(10000, len(df_with_narratives))\n",
    "sample_df = df_with_narratives.sample(n=sample_size, random_state=42)\n",
    "\n",
    "# Tokenize all narratives\n",
    "all_tokens = []\n",
    "for narrative in sample_df[narrative_col]:\n",
    "    tokens = clean_and_tokenize(narrative)\n",
    "    all_tokens.extend(tokens)\n",
    "\n",
    "# Get most common words\n",
    "word_freq = Counter(all_tokens)\n",
    "top_words = word_freq.most_common(30)\n",
    "\n",
    "print(\"Top 30 most common words:\")\n",
    "for word, count in top_words:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# Create word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud of Complaint Narratives')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e258f893",
   "metadata": {},
   "source": [
    "## 4. Data Cleaning and Preprocessing\n",
    "\n",
    "Now let's clean the text narratives to improve embedding quality and prepare the final dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c523fc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning function\n",
    "def clean_complaint_text(text):\n",
    "    \"\"\"\n",
    "    Clean complaint text for better embedding quality\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return None\n",
    "    \n",
    "    # Convert to string and lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Remove common boilerplate text\n",
    "    boilerplate_phrases = [\n",
    "        \"i am writing to file a complaint\",\n",
    "        \"i would like to file a complaint\",\n",
    "        \"this is a complaint about\",\n",
    "        \"dear sir or madam\",\n",
    "        \"to whom it may concern\",\n",
    "        \"i am contacting you regarding\",\n",
    "        \"i am writing this letter to\",\n",
    "        \"xxxx\", \"xx/xx/xxxx\"\n",
    "    ]\n",
    "    \n",
    "    for phrase in boilerplate_phrases:\n",
    "        text = text.replace(phrase, \"\")\n",
    "    \n",
    "    # Remove excessive whitespace and normalize\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Remove very short texts (less than 20 characters)\n",
    "    if len(text) < 20:\n",
    "        return None\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply cleaning\n",
    "print(\"=== CLEANING COMPLAINT NARRATIVES ===\")\n",
    "print(f\"Before cleaning: {len(df_with_narratives)} complaints\")\n",
    "\n",
    "df_with_narratives['cleaned_narrative'] = df_with_narratives[narrative_col].apply(clean_complaint_text)\n",
    "\n",
    "# Remove rows with None values after cleaning\n",
    "df_cleaned = df_with_narratives.dropna(subset=['cleaned_narrative']).copy()\n",
    "\n",
    "print(f\"After cleaning: {len(df_cleaned)} complaints\")\n",
    "print(f\"Removed: {len(df_with_narratives) - len(df_cleaned)} complaints\")\n",
    "\n",
    "# Compare original vs cleaned text length\n",
    "df_cleaned['original_length'] = df_cleaned[narrative_col].str.len()\n",
    "df_cleaned['cleaned_length'] = df_cleaned['cleaned_narrative'].str.len()\n",
    "\n",
    "print(\"\\n=== TEXT LENGTH COMPARISON ===\")\n",
    "print(\"Original text length:\")\n",
    "print(df_cleaned['original_length'].describe())\n",
    "print(\"\\nCleaned text length:\")\n",
    "print(df_cleaned['cleaned_length'].describe())\n",
    "\n",
    "# Show example of cleaning\n",
    "print(\"\\n=== CLEANING EXAMPLE ===\")\n",
    "idx = 0\n",
    "print(\"Original:\")\n",
    "print(df_cleaned[narrative_col].iloc[idx])\n",
    "print(\"\\nCleaned:\")\n",
    "print(df_cleaned['cleaned_narrative'].iloc[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b544fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare final dataset\n",
    "print(\"=== PREPARING FINAL DATASET ===\")\n",
    "\n",
    "# Select relevant columns for the RAG system\n",
    "columns_to_keep = [\n",
    "    'Complaint ID',\n",
    "    'Product_mapped',\n",
    "    'Issue',\n",
    "    'Sub-issue',\n",
    "    'Company',\n",
    "    'State',\n",
    "    'Date received',\n",
    "    'cleaned_narrative',\n",
    "    'original_length',\n",
    "    'cleaned_length'\n",
    "]\n",
    "\n",
    "# Create final dataset\n",
    "final_df = df_cleaned[columns_to_keep].copy()\n",
    "final_df = final_df.rename(columns={\n",
    "    'Product_mapped': 'Product',\n",
    "    'cleaned_narrative': 'Consumer_complaint_narrative'\n",
    "})\n",
    "\n",
    "print(f\"Final dataset shape: {final_df.shape}\")\n",
    "print(f\"Final dataset columns: {final_df.columns.tolist()}\")\n",
    "\n",
    "# Final statistics by product\n",
    "print(\"\\n=== FINAL DATASET STATISTICS BY PRODUCT ===\")\n",
    "product_stats = final_df.groupby('Product').agg({\n",
    "    'Consumer_complaint_narrative': 'count',\n",
    "    'cleaned_length': ['mean', 'median', 'std']\n",
    "}).round(2)\n",
    "\n",
    "product_stats.columns = ['Count', 'Mean_Length', 'Median_Length', 'Std_Length']\n",
    "print(product_stats)\n",
    "\n",
    "# Save the cleaned dataset\n",
    "output_path = \"../data/filtered_complaints.csv\"\n",
    "final_df.to_csv(output_path, index=False)\n",
    "print(f\"\\nâœ… Cleaned dataset saved to: {output_path}\")\n",
    "\n",
    "# Summary report\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SUMMARY REPORT\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Original dataset size: {len(df):,} complaints\")\n",
    "print(f\"After product filtering: {len(df_filtered):,} complaints ({len(df_filtered)/len(df)*100:.1f}%)\")\n",
    "print(f\"After narrative filtering: {len(df_with_narratives):,} complaints ({len(df_with_narratives)/len(df)*100:.1f}%)\")\n",
    "print(f\"Final cleaned dataset: {len(final_df):,} complaints ({len(final_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"\\nTarget products included:\")\n",
    "for product in final_df['Product'].unique():\n",
    "    count = len(final_df[final_df['Product'] == product])\n",
    "    print(f\"  - {product}: {count:,} complaints\")\n",
    "print(f\"\\nAverage narrative length: {final_df['cleaned_length'].mean():.0f} characters\")\n",
    "print(f\"Ready for embedding and vector store creation! ðŸš€\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
